{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import os\n",
    "import copy\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from operator import itemgetter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT DATA FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add data files location/workingdirectory here\n",
    "\n",
    "def init_data(location=None):\n",
    "    data=list()#################################################### list to store strings of txt file\n",
    "    if location==None:\n",
    "        location = os.getcwd()##################################### get current working directory location here    \n",
    "    for file in os.listdir(location):\n",
    "        try:\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(file,\"r\",encoding=\"utf-8\") as f:\n",
    "                    data.append(f.read())\n",
    "        except Exception as e:\n",
    "            raise \"No files found\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLEANNING AND N-GRAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data=None):\n",
    "    if data==None or not data:\n",
    "        print(\"No data to clean\")\n",
    "    else:\n",
    "        try:################# can be shortened by regex ###########################\n",
    "            final_token_stream=[]\n",
    "            working_data=data.copy() ################################### WORKING DATA ######################################\n",
    "            stoplist = set(stopwords.words('english') + list(punctuation) + list(['“','”']))\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "            clean_data=[]\n",
    "            for i in working_data:\n",
    "                i=i.replace(\"\\ufeff\",\"\").replace(\"\\t\",\" \").replace(\"_\",\"\").replace(\".\",\"\").replace(\",\",\"\").strip()\n",
    "                #i=i.replace(\"'ll\",\" will\").replace(\"'s\",\" is\").replace(\"'re\",\" are\").replace(\"'m\",\" am\")\n",
    "                i=re.sub(\"[\\n]+\\s?\",\" \",i)\n",
    "                i=i.lower();\n",
    "                clean_data.append(i)\n",
    "            for i in clean_data:\n",
    "                out=nlp(i)\n",
    "                #for token in out:\n",
    "                 #   print(token, token.pos_, token.lemma_, token.text);\n",
    "    # BUILD PROPERNOUN AND ADVERBS LIST #\n",
    "                proper_noun=set([token.text for token in out if token.pos_ == \"PROPN\"])\n",
    "                adverb=set([token.text for token in out if token.pos_ == \"ADV\"])\n",
    "    # BUILD ELIMINATION LIST OF WORDS #\n",
    "                eliminate=[]\n",
    "                eliminate.extend(stoplist)\n",
    "                eliminate.extend(list(proper_noun))\n",
    "                eliminate.extend(list(adverb))\n",
    "                #print(eliminate);\n",
    "    # TOKENS AFTER ELIMINATION #\n",
    "                new_token_stream=\"\"\n",
    "                for token in out:\n",
    "                    if (token.pos_!=\"PUNCT\" and token.pos_!=\"SPACE\" and token.lemma_!=\"-PRON-\"):\n",
    "                        new_token_stream+=token.lemma_+\" \"\n",
    "                    elif token.lemma_==\"-PRON-\":\n",
    "                        new_token_stream+=token.text+\" \"\n",
    "                        \n",
    "                tokens = [token for token in nltk.word_tokenize(new_token_stream) if token not in eliminate]\n",
    "                final_token_stream.append(tokens)\n",
    "                #print(new_token_stream);\n",
    "            return final_token_stream\n",
    "        except Exception as e:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build HASH table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hash_table(data=None):\n",
    "    if data==None or (not data):\n",
    "        print(\"No data to build table\")\n",
    "    else:\n",
    "        try:\n",
    "            grams_hash = {}\n",
    "            for i in data:\n",
    "                three_grams_list.extend(list(ngrams(i, 3)))\n",
    "            grams_hash=Counter(three_grams_list)\n",
    "           # sorted_grams_hash=sorted(grams_hash.items(), key=itemgetter(1),reverse=True)\n",
    "            return grams_hash\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        '''try:\n",
    "            grams_hash = {}\n",
    "            for i in data:\n",
    "                three_grams_list=list(ngrams(i, 3))\n",
    "                for gram in three_grams_list:\n",
    "                    if gram not in grams_hash.keys():\n",
    "                        grams_hash.update({gram:1})\n",
    "                    else:\n",
    "                        gram_occurrences = grams_hash[gram]\n",
    "                        grams_hash.update({gram:gram_occurrences+1})\n",
    "            sorted_grams_hash=sorted(grams_hash.items(), key=itemgetter(1),reverse=True)\n",
    "            return sorted_grams_hash'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unstable takes long time, needs more thought, wrongly coded\n",
    "def cosine_similarity(a, b):\n",
    "    for i in cleaned_test:\n",
    "        a.extend(list(ngrams(i, 3)))\n",
    "    vec1 = Counter(a)\n",
    "    vec2 = b\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR TESTING ONLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHODS DEFINED\n",
    "#### -> init\\_data(foo)  : \"foo\" type:string, location of working directory; returns list of strings\n",
    "#### -> clean(foo)  : \"foo\" type:list of obtained data strings from init\\_data; returns list of cleaned  strings\n",
    "#### -> build\\_hash\\_table(foo) : \"foo\" type:list of obtained tokenised strings from clean(); returns dictionary having key:value\\::token:count desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$TEST$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=init_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data=clean(data)\n",
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_table=build_hash_table(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100320"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.Counter"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string=[\"He had concluded that pigs must be able to fly in Hog Heaven.\",\"I would be delighted if the sea were full of cucumber juice.\",\\\n",
    "             \"If I don’t like something, I’ll stay away from it.\",\"At that moment she realized she had a sixth sense.\"\\\n",
    "             \"Italy is my favorite country; in fact, I plan to spend two weeks there next year.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He had concluded that pigs must be able to fly in Hog Heaven.',\n",
       " 'I would be delighted if the sea were full of cucumber juice.',\n",
       " 'If I don’t like something, I’ll stay away from it.',\n",
       " 'At that moment she realized she had a sixth sense.Italy is my favorite country; in fact, I plan to spend two weeks there next year.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_test=clean(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['conclude', 'pig', 'must', 'able', 'fly'],\n",
       " ['would', 'delight', 'sea', 'full', 'cucumber', 'juice'],\n",
       " ['like', 'something', 'stay'],\n",
       " ['moment',\n",
       "  'realize',\n",
       "  'sixth',\n",
       "  'senseitaly',\n",
       "  'favorite',\n",
       "  'country',\n",
       "  'fact',\n",
       "  'plan',\n",
       "  'spend',\n",
       "  'two',\n",
       "  'week',\n",
       "  'next',\n",
       "  'year']]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cs=cosine_similarity(cleaned_test,hash_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('conclude', 'pig', 'must'): 1,\n",
       "         ('pig', 'must', 'able'): 1,\n",
       "         ('must', 'able', 'fly'): 1,\n",
       "         ('would', 'delight', 'sea'): 1,\n",
       "         ('delight', 'sea', 'full'): 1,\n",
       "         ('sea', 'full', 'cucumber'): 1,\n",
       "         ('full', 'cucumber', 'juice'): 1,\n",
       "         ('like', 'something', 'stay'): 1,\n",
       "         ('moment', 'realize', 'sixth'): 1,\n",
       "         ('realize', 'sixth', 'senseitaly'): 1,\n",
       "         ('sixth', 'senseitaly', 'favorite'): 1,\n",
       "         ('senseitaly', 'favorite', 'country'): 1,\n",
       "         ('favorite', 'country', 'fact'): 1,\n",
       "         ('country', 'fact', 'plan'): 1,\n",
       "         ('fact', 'plan', 'spend'): 1,\n",
       "         ('plan', 'spend', 'two'): 1,\n",
       "         ('spend', 'two', 'week'): 1,\n",
       "         ('two', 'week', 'next'): 1,\n",
       "         ('week', 'next', 'year'): 1})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "for i in cleaned_test:\n",
    "    a.extend(list(ngrams(i, 3)))\n",
    "vec1 = Counter(a)\n",
    "vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
